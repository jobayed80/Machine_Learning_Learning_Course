{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMMHO+QwaqJ2zZph/tpMhRZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jobayed80/Machine_Learning_Learning_Course/blob/main/DS_424_Lab_Assignments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build an image classification model and predict MNIST Digits using Wide and Deep Neural Network. Your should use callback functions to implement early stopping and save the best model into appropriate format. Report the training and test accuracy and other evaluation metrics. **\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "onayoBnJAyjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers, models, callbacks, initializers , regularizers, optimizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img, ImageDataGenerator\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense\n"
      ],
      "metadata": {
        "id": "5mYjvKnf1HPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n"
      ],
      "metadata": {
        "id": "QTFCwCJO-BRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "metadata": {
        "id": "Je_7fK1--DC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to build the Wide and Deep Neural Network model with specified activations and initializers\n",
        "def build_wide_and_deep_model(activation='relu', initializer='glorot_uniform'):\n",
        "    input_layer = layers.Input(shape=(28, 28, 1))\n",
        "\n",
        "    # Wide Path\n",
        "    flat = layers.Flatten()(input_layer)\n",
        "    wide = layers.Dense(64, activation=activation, kernel_initializer=initializer)(flat)\n",
        "\n",
        "    # Deep Path\n",
        "    conv1 = layers.Conv2D(32, (3, 3), activation=activation, kernel_initializer=initializer)(input_layer)\n",
        "    pool1 = layers.MaxPooling2D((2, 2))(conv1)\n",
        "    conv2 = layers.Conv2D(64, (3, 3), activation=activation, kernel_initializer=initializer)(pool1)\n",
        "    pool2 = layers.MaxPooling2D((2, 2))(conv2)\n",
        "    flat = layers.Flatten()(pool2)\n",
        "    deep = layers.Dense(64, activation=activation, kernel_initializer=initializer)(flat)\n",
        "\n",
        "    # Merge Wide and Deep paths\n",
        "    merge = layers.concatenate([wide, deep])\n",
        "    output = layers.Dense(10, activation='softmax')(merge)\n",
        "\n",
        "    model = models.Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZQbEWDUj-FOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use the same datasets and model as above, but this time use different activation functions and corresponding weight initialization methods to see if there is any difference from the perspective of epochs, errors, accuracy, etc. **\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Y9Xolst3-dl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation functions to try\n",
        "activations = ['relu', 'sigmoid', 'tanh']\n",
        "\n",
        "# Weight initialization methods to try\n",
        "initializers = ['glorot_uniform', 'he_normal', 'lecun_normal']\n"
      ],
      "metadata": {
        "id": "SY9UACMi-Vnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_wide_and_deep_model()\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "ZrEMkGGQDrz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callback functions\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "model_checkpoint = callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n"
      ],
      "metadata": {
        "id": "DnQbIORqEGGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ps_Pl4LUFsAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model with callback functions\n",
        "history = model.fit(train_images, train_labels, epochs=2, batch_size=64,\n",
        "                    validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n",
        "\n"
      ],
      "metadata": {
        "id": "Gf3gUYtd4o0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "\n",
        "# Print test accuracy and other evaluation metrics\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Test Loss: {test_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "V5X4LutVFoB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can load the best model from the saved checkpoint\n",
        "best_model = models.load_model('best_model.h5')"
      ],
      "metadata": {
        "id": "nSIxO-PqFw4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set using the best model\n",
        "predictions = best_model.predict(test_images)"
      ],
      "metadata": {
        "id": "53vXTyCVFxvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the results\n",
        "for i in range(5):\n",
        "    plt.imshow(test_images[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(f'Predicted: {predictions[i]}, Actual: {tf.argmax(test_labels[i])}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fm2x7xyzEKDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mAtmIKYxFXjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use the same dataset, model, activation functions, and weight initialization methods. However, now try different optimizers such as Momentum, Nesterov, AdaGrad, RMSProp, ADAM, and so on, and check whether you can reduce the training time, or increase the accuracy.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "m1QTOfkhGxH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizers to try\n",
        "optimizers = ['sgd', 'momentum', 'nesterov', 'adagrad', 'rmsprop', 'adam']"
      ],
      "metadata": {
        "id": "uqdTf8Y-Gxwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a Wide and Deep Neural Network\n",
        "model = models.Sequential()\n",
        "model.add(layers.Flatten(input_shape=(28, 28, 1)))\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "sTttw4y95hRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with different optimizers\n",
        "optimizers_list = ['sgd', 'adagrad', 'rmsprop', 'adam']"
      ],
      "metadata": {
        "id": "VlOXqyaD5rvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for optimizer_name in optimizers_list:\n",
        "    model.compile(optimizer=optimizer_name,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "x2_l63WEGyS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Train the model\n",
        "    historys = model.fit(train_images, train_labels, epochs=2, batch_size=64, validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "id": "_Fj25VCe6aE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "    print(f\"\\nOptimizer: {optimizer_name}\")\n",
        "    print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "6QSWGpbj6V-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the training process\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')"
      ],
      "metadata": {
        "id": "4msP2RkI64h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4\tUse the same dataset, model, activation functions, weight initialization methods, and optimizer as before. However, this time implement different regularization methods such as L1, L2, and Dropout to reduce overfitting. See Chapter 12 of the Textbook 1.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BxQkg17Ryjfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a Wide and Deep Neural Network with regularization\n",
        "model = models.Sequential()\n",
        "model.add(layers.Flatten(input_shape=(28, 28, 1)))\n",
        "model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "GO6vcB9lylh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with an optimizer\n",
        "optimizer_name = 'adam'\n",
        "model.compile(optimizer=optimizer_name,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "D0BZEX96yoH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(train_images, train_labels, epochs=2, batch_size=64, validation_data=(test_images, test_labels))\n"
      ],
      "metadata": {
        "id": "fFmFwhnEyxAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"\\nOptimizer: {optimizer_name}\")\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "9g5U6xzBy0Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the training process\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')"
      ],
      "metadata": {
        "id": "r0mifa9l6hjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jGh4-pc66p8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement ResNet-34 architecture from the scratch using Keras Sequential API. Now train the network to predict on MNIST Fashion dataset. Evaluate your model using appropriate metrics. **\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "h9ohMSOX0pzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Residual Block\n",
        "def residual_block(x, filters, kernel_size=3, strides=1):\n",
        "    y = layers.Conv2D(filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    y = layers.Activation('relu')(y)\n",
        "\n",
        "    y = layers.Conv2D(filters, kernel_size=kernel_size, strides=strides, padding='same')(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "\n",
        "    # Add the shortcut connection\n",
        "    if strides > 1:\n",
        "        x = layers.Conv2D(filters, kernel_size=1, strides=strides, padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Add the shortcut connection to the main path\n",
        "    y = layers.add([x, y])\n",
        "    y = layers.Activation('relu')(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "-ECHO3mKz_Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the ResNet-34 model\n",
        "model = models.Sequential()"
      ],
      "metadata": {
        "id": "ygX6-72k0wu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Convolutional Layer\n",
        "model.add(layers.Conv2D(64, kernel_size=7, strides=2, input_shape=(28, 28, 1), padding='same'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=3, strides=2, padding='same'))"
      ],
      "metadata": {
        "id": "Wkx7A7PS0ym6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual Stages\n",
        "stage_filters = [64, 128, 256, 512]\n",
        "for filters in stage_filters:\n",
        "    model.add(residual_block(model.layers[-1].output, filters, strides=2))"
      ],
      "metadata": {
        "id": "c6yVLZ_v7U6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Average Pooling and Dense Layer\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "model.add(layers.Dense(10, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "ewnRPTMC024s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "gVwMYRHm1WaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "history = model.fit(train_images, train_labels, epochs=2, batch_size=64, validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "id": "AEuWDMLq1YuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "WdCyhVAv1cfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the training process\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')"
      ],
      "metadata": {
        "id": "gr7aSoxBE46x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rjAXnv537v2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Now train the same dataset and the same ResNet-34, but use pre-trained weights. In other words, use transfer learning to train the same model and determine whether there is any improvement in training time, accuracy, or any other aspect.\n",
        "\n",
        "---\n",
        "\n",
        "**bold text**"
      ],
      "metadata": {
        "id": "kUkQDFqs7Ytk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeat single-channel images to have three channels\n",
        "train_images_rgb = tf.image.grayscale_to_rgb(tf.convert_to_tensor(train_images))\n",
        "test_images_rgb = tf.image.grayscale_to_rgb(tf.convert_to_tensor(test_images))\n"
      ],
      "metadata": {
        "id": "0xhCK7u73D-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize images to meet the minimum size required by ResNet50\n",
        "train_images_rgb = tf.image.resize(train_images_rgb, (32, 32))\n",
        "test_images_rgb = tf.image.resize(test_images_rgb, (32, 32))\n"
      ],
      "metadata": {
        "id": "I3E9tG7d7kIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the base ResNet-50 model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n"
      ],
      "metadata": {
        "id": "Xp09K1Ju7mJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the layers of the pre-trained model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n"
      ],
      "metadata": {
        "id": "AqaAu3A77n70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the transfer learning model\n",
        "model = models.Sequential()\n",
        "model.add(base_model)\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "model.add(layers.Dense(10, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "x5oSpaGn7zfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "39E7P6Vr70_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.1\n",
        ")\n"
      ],
      "metadata": {
        "id": "vXsOZQzO73Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen.fit(train_images_rgb)"
      ],
      "metadata": {
        "id": "vJ0dzos88d-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(datagen.flow(train_images_rgb, train_labels, batch_size=32),\n",
        "          epochs=1,\n",
        "          validation_data=(test_images_rgb, test_labels))"
      ],
      "metadata": {
        "id": "4grwTAxN8E9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images_rgb, test_labels)\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "gh7NkIdF8h2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the training process\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')"
      ],
      "metadata": {
        "id": "MFP9h6vYwwTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xWBZjFJ38zHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Generate an artificial time series data and predict it using simple, deep Recurrent Neural Network, or LSTM cells.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OXcxITyZ9LDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate time series data\n",
        "def generate_time_series(signal_length=1000):\n",
        "    time = np.arange(0, signal_length)\n",
        "    signal = np.sin(0.1 * time) + 0.2 * np.random.randn(signal_length)\n",
        "    return signal"
      ],
      "metadata": {
        "id": "6loXtuph85hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create sequences for time series prediction\n",
        "def create_sequences(data, sequence_length):\n",
        "    sequences, targets = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        seq = data[i : i + sequence_length]\n",
        "        target = data[i + sequence_length]\n",
        "        sequences.append(seq)\n",
        "        targets.append(target)\n",
        "    return np.array(sequences), np.array(targets)"
      ],
      "metadata": {
        "id": "ZrllHIpHJHTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate time series data\n",
        "signal_length = 1000\n",
        "signal = generate_time_series(signal_length)\n"
      ],
      "metadata": {
        "id": "38MUA-RSJJc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set sequence length for prediction\n",
        "sequence_length = 20\n"
      ],
      "metadata": {
        "id": "nI5NaAxQJMJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequences for training\n",
        "sequences, targets = create_sequences(signal, sequence_length)\n"
      ],
      "metadata": {
        "id": "rb0223aqJVxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(sequences, targets, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Et0j4_vQJXOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the data for RNN and LSTM models\n",
        "X_train = X_train.reshape(-1, sequence_length, 1)\n",
        "X_test = X_test.reshape(-1, sequence_length, 1)\n"
      ],
      "metadata": {
        "id": "10hZPi7_JabX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Simple RNN model\n",
        "model_rnn = Sequential([\n",
        "    SimpleRNN(50, input_shape=(sequence_length, 1)),\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "w-Zqk17rJmD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn.compile(optimizer='adam', loss='mean_squared_error')\n"
      ],
      "metadata": {
        "id": "QQ6NFXYxJnoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Simple RNN model\n",
        "history_rnn = model_rnn.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), verbose=2)\n"
      ],
      "metadata": {
        "id": "_usUiCDlJpY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build LSTM model\n",
        "model_lstm = Sequential([\n",
        "    LSTM(50, input_shape=(sequence_length, 1)),\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "3QsLqKH1JrBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n"
      ],
      "metadata": {
        "id": "PZJTk_G8Jufg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LSTM model\n",
        "history_lstm = model_lstm.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), verbose=2)"
      ],
      "metadata": {
        "id": "-xGmyEpYJw7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the training loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_rnn.history['loss'], label='RNN Training Loss')\n",
        "plt.plot(history_rnn.history['val_loss'], label='RNN Validation Loss')\n",
        "plt.title('Simple RNN Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "XvsPId2FJzG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict using both models\n",
        "X_test_reshaped = X_test.reshape((-1, sequence_length, 1))\n",
        "\n",
        "# Predictions using Simple RNN\n",
        "predictions_rnn = model_rnn.predict(X_test_reshaped)\n",
        "\n",
        "# Predictions using LSTM\n",
        "predictions_lstm = model_lstm.predict(X_test_reshaped)\n"
      ],
      "metadata": {
        "id": "2fTWydBQKQ_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the predictions\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test, label='True Values', linestyle='dashed', color='black')\n",
        "plt.plot(predictions_rnn, label='RNN Predictions', linestyle='dashed', color='blue')\n",
        "plt.plot(predictions_lstm, label='LSTM Predictions', linestyle='dashed', color='red')\n",
        "plt.title('Time Series Prediction with RNN and LSTM')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Values')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VuBfeg9SKVvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NpPDUi5ZKnb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Generate Shakespearean texts using Character Recurrent Neural Network. Develop the training dataset by splitting and chopping the training texts. Then develop a Char-RNN model to generate fake Shakespearean texts.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YopGFratKrHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import requests\n",
        "import string"
      ],
      "metadata": {
        "id": "0qUextY0Ku-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Shakespearean text\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "shakespeare_text = requests.get(url).text"
      ],
      "metadata": {
        "id": "A5mRrDTrrzi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text\n",
        "shakespeare_text = shakespeare_text.lower()\n",
        "shakespeare_text = shakespeare_text.translate(str.maketrans(\"\", \"\", string.punctuation))"
      ],
      "metadata": {
        "id": "AchrKV6sr96R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create character-to-index and index-to-character mappings\n",
        "chars = sorted(list(set(shakespeare_text)))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(chars)}"
      ],
      "metadata": {
        "id": "y5WRTIaTsHXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequences for training\n",
        "seq_length = 100\n",
        "step = 1\n",
        "sequences = []\n",
        "next_chars = []"
      ],
      "metadata": {
        "id": "-t2MXCI6sLTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(shakespeare_text) - seq_length, step):\n",
        "    seq = shakespeare_text[i : i + seq_length]\n",
        "    next_char = shakespeare_text[i + seq_length]\n",
        "    sequences.append(seq)\n",
        "    next_chars.append(next_char)"
      ],
      "metadata": {
        "id": "FScVyXcBsOr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sequences to input and target data\n",
        "X = np.zeros((len(sequences), seq_length, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, seq in enumerate(sequences):\n",
        "    for t, char in enumerate(seq):\n",
        "        X[i, t, char_to_idx[char]] = 1\n",
        "    y[i, char_to_idx[next_chars[i]]] = 1"
      ],
      "metadata": {
        "id": "qhhndM-tsSRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Char-RNN model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(128, input_shape=(seq_length, len(chars))),\n",
        "    tf.keras.layers.Dense(len(chars), activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")"
      ],
      "metadata": {
        "id": "loEI2flcsXBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using the trained model\n",
        "def generate_text(seed_text, length=500, temperature=1.0):\n",
        "    generated_text = seed_text\n",
        "\n",
        "    for _ in range(length):\n",
        "        x_pred = np.zeros((1, seq_length, len(chars)))\n",
        "\n",
        "        for t, char in enumerate(seed_text):\n",
        "            x_pred[0, t, char_to_idx[char]] = 1.0\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds, temperature)\n",
        "        next_char = idx_to_char[next_index]\n",
        "\n",
        "        generated_text += next_char\n",
        "        seed_text = seed_text[1:] + next_char\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "RlZGQKYqtBLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to sample from the predicted probabilities\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "10V5JopzuHbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Shakespearean text\n",
        "seed_text = \"shall i compare thee to a summers day\\n\"\n",
        "generated_text = generate_text(seed_text, length=1000, temperature=0.5)"
      ],
      "metadata": {
        "id": "YTRtsNlAuLsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print and save the generated text\n",
        "print(generated_text)\n",
        "\n",
        "# Save the model for future use\n",
        "model.save(\"char_rnn_shakespeare_model.h5\")"
      ],
      "metadata": {
        "id": "pYdegUNmuXMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Download the IMBD dataset and perform a sentiment analysis on the movie reviews. You can use pre-trained embeddings if you want.\n",
        "\n",
        "---\n",
        "\n",
        "**bold text**"
      ],
      "metadata": {
        "id": "W2KOUy9Hwjjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import get_file\n"
      ],
      "metadata": {
        "id": "lc-eJd5Cwl4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and load the IMDB dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n"
      ],
      "metadata": {
        "id": "kDExVod05Y8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the GloVe word embeddings\n",
        "# Specify the correct path to the extracted glove file\n",
        "glove_txt_path = '/path/to/glove.6B.100d.txt'  # Update with the actual path to your glove file\n",
        "\n",
        "embedding_index = {}\n",
        "with open(glove_txt_path) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "        embedding_index[word] = coefs\n"
      ],
      "metadata": {
        "id": "LBxnhP3_Asim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an embedding matrix\n",
        "word_index = imdb.get_word_index()\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "IURAaPT35fae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences to a fixed length\n",
        "max_len = 200\n",
        "x_train_padded = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test_padded = pad_sequences(x_test, maxlen=max_len)\n"
      ],
      "metadata": {
        "id": "NRYpq52r9RUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))"
      ],
      "metadata": {
        "id": "RAaZyIz_AiMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "oW3cAupFAlOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(x_train_padded, y_train, epochs=2, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "nZCv0qkDAqcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the model on test data\n",
        "loss, accuracy = model.evaluate(x_test_padded, y_test)\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "XC8z_WUJAsJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize training process\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lbqMpdkLD0Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Suppose you are going to classify images of at cats and dogs. Now collect images of different cats and dogs from online or real life. You and your classmates may develop this dataset together. Then organize the dataset in ImageNet folder structure.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-kuZF6J_B_LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras import layers\n",
        "import cv2, os\n",
        "from tqdm import tqdm\n",
        "from random import shuffle\n",
        "import shutil"
      ],
      "metadata": {
        "id": "_BJ_YA5CRNkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://cainvas-static.s3.amazonaws.com/media/user_data/tanmay/dogvcat.zip\"\n",
        "!unzip -qo dogvcat.zip"
      ],
      "metadata": {
        "id": "GaB8FAm4HYQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir= \"./datset/training\"\n",
        "test_dir= \"./datset/testing/\"\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split= 0.2)\n",
        "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(50,50), color_mode='grayscale', batch_size=20, class_mode='binary', subset= 'training')\n",
        "val_generator = train_datagen.flow_from_directory(train_dir, target_size=(50,50), color_mode='grayscale', batch_size=20, class_mode='binary', subset= 'validation')"
      ],
      "metadata": {
        "id": "H1S1ABFjRRJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(16, kernel_size=(3,3), activation='relu',input_shape=(50,50,1), padding='same'))\n",
        "# model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "# model.add(tf.keras.activations.relu(alpha=0.1))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "# model.add(tf.keras.activations.relu(alpha=0.1))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "# model.add(tf.keras.activations.relu(alpha=0.1))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
        "# model.add(tf.keras.activations.relu(alpha=0.1))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "rYrinx1mRVlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Compilation\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit_generator(train_generator, epochs=2, validation_data=val_generator, validation_steps=50)"
      ],
      "metadata": {
        "id": "CHjm9NqpBEkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XwEYleN8Raeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w9JOezBCRacN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8dGGY_2RaXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11\tUse the TensorFlow Data API to load your dataset that you developed in the previous section. Using appropriate preprocessing and data augmentation techniques as necessary.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5rkN-xFrMDRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "HlnyomrYMO9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_ds, val_ds, test_ds), metadata = tfds.load(\n",
        "    'tf_flowers',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")"
      ],
      "metadata": {
        "id": "GRQy_jBPMqDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = metadata.features['label'].num_classes\n",
        "print(num_classes)"
      ],
      "metadata": {
        "id": "GYLJo4o0MuEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_label_name = metadata.features['label'].int2str\n",
        "\n",
        "image, label = next(iter(train_ds))\n",
        "_ = plt.imshow(image)\n",
        "_ = plt.title(get_label_name(label))"
      ],
      "metadata": {
        "id": "VnDLgOJYMxGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "])"
      ],
      "metadata": {
        "id": "Hg6oBfyBMzou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the image to a batch.\n",
        "image = tf.cast(tf.expand_dims(image, 0), tf.float32)"
      ],
      "metadata": {
        "id": "vC7wGvvqM83D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "  augmented_image = data_augmentation(image)\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  plt.imshow(augmented_image[0])\n",
        "  plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "Dhx-LNRUM_NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_invert_img(x, p=0.5):\n",
        "  if  tf.random.uniform([]) < p:\n",
        "    x = (255-x)\n",
        "  else:\n",
        "    x\n",
        "  return x"
      ],
      "metadata": {
        "id": "yuz_zn6TNA42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_invert(factor=0.5):\n",
        "  return layers.Lambda(lambda x: random_invert_img(x, factor))\n",
        "\n",
        "random_invert = random_invert()"
      ],
      "metadata": {
        "id": "kjzSOiOFNKPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "  augmented_image = random_invert(image)\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  plt.imshow(augmented_image[0].numpy().astype(\"uint8\"))\n",
        "  plt.axis(\"on\")"
      ],
      "metadata": {
        "id": "MrzKab9MNLuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_ds, val_ds, test_ds), metadata = tfds.load(\n",
        "    'tf_flowers',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")"
      ],
      "metadata": {
        "id": "ER541I-YNNar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = next(iter(train_ds))\n",
        "_ = plt.imshow(image)\n",
        "_ = plt.title(get_label_name(label))"
      ],
      "metadata": {
        "id": "hdqZgWgSNTQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grayscaled = tf.image.rgb_to_grayscale(image)\n",
        "visualize(image, tf.squeeze(grayscaled))\n",
        "_ = plt.colorbar()"
      ],
      "metadata": {
        "id": "mMApYPj6N-NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Develop at least 3 different CNN architectures such as VGG, ResNet, MobileNet, etc., using Keras and train your model on the dataset. Report the appropriate evaluation metrics of your models.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qwJbH_IwCRQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras import layers\n",
        "import cv2, os\n",
        "from tqdm import tqdm\n",
        "from random import shuffle\n",
        "import shutil"
      ],
      "metadata": {
        "id": "ox2H-5XaOCV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://cainvas-static.s3.amazonaws.com/media/user_data/tanmay/dogvcat.zip\"\n",
        "!unzip -qo dogvcat.zip"
      ],
      "metadata": {
        "id": "e93ttxtrh69V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir= \"./datset/training\"\n",
        "test_dir= \"./datset/testing/\"\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split= 0.2)\n",
        "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(50,50), color_mode='grayscale', batch_size=20, class_mode='binary', subset= 'training')\n",
        "val_generator = train_datagen.flow_from_directory(train_dir, target_size=(50,50), color_mode='grayscale', batch_size=20, class_mode='binary', subset= 'validation')"
      ],
      "metadata": {
        "id": "ONEgbzXGiDd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(16, kernel_size=(3,3), activation='relu',input_shape=(50,50,1), padding='same'))\n",
        "# model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "# model.add(tf.keras.activations.relu(alpha=0.1))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "# model.add(tf.keras.activations.relu(alpha=0.1))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "# model.add(tf.keras.activations.relu(alpha=0.1))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
        "# model.add(tf.keras.activations.relu(alpha=0.1))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "UKRA-8KtiGvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Compilation\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit_generator(train_generator, epochs=5, validation_data=val_generator, validation_steps=50)\n"
      ],
      "metadata": {
        "id": "NzrXGx11iIuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "58IN1YYZiMZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DpKaWEd4iRAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K7Sznh78kbPk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}